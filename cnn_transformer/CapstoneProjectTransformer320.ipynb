{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZISvjEpzMPK",
        "outputId": "86153b06-ddfc-40f7-e85b-f1a71ee9c805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (1.3.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n",
            "Mounted at /content/drive\n",
            "Success! Connected to Capstone Data.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install the specific library for the CNN backbone\n",
        "!pip install timm\n",
        "\n",
        "# 2. Connect to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Verify you can see your files (Change path if needed)\n",
        "import os\n",
        "# Check if your folder exists\n",
        "if os.path.exists('/content/drive/MyDrive/Seneca/Capstone/spectrograms'):\n",
        "    print(\"Success! Connected to Capstone Data.\")\n",
        "else:\n",
        "    print(\"Warning: Folder not found. Check your path in Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDM_Vo612ZyC",
        "outputId": "2ab69752-aa98-40fe-db65-be883450fd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "zip_path = '/content/drive/MyDrive/Seneca/Capstone/spectrograms/train_spectrograms.zip'\n",
        "local_dir = '/content/train_spectrograms'\n",
        "\n",
        "print(\"Copying data to local machine (this speeds up training)...\")\n",
        "# Unzip directly to local folder\n",
        "shutil.unpack_archive(zip_path, local_dir)\n",
        "\n",
        "print(\"Done! Data is ready for the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY1KGNSj2KBe",
        "outputId": "9c3b04e9-978e-4940-93d6-9fbb48929569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying data to local machine (this speeds up training)...\n",
            "Done! Data is ready for the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the Validation Spectrograms\n",
        "val_zip_path = '/content/drive/MyDrive/Seneca/Capstone/spectrograms/val_spectrograms.zip'\n",
        "val_local_dir = '/content/val_spectrograms'\n",
        "\n",
        "print(\"Unzipping validation data...\")\n",
        "shutil.unpack_archive(val_zip_path, val_local_dir)\n",
        "print(\"Done! Validation data ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkNt4MWW_KWV",
        "outputId": "ddf986ff-71c0-4ab2-d10b-004e3b774935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping validation data...\n",
            "Done! Validation data ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HMSDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        spec_id = row['spectrogram_id']\n",
        "        img_path = f\"{self.img_dir}/{spec_id}.parquet\"\n",
        "\n",
        "        try:\n",
        "            # Load data (Time is usually the index, Frequency is columns)\n",
        "            # Shape: (Time_Steps, Frequencies) e.g., (4000, 400)\n",
        "            spectrogram = pd.read_parquet(img_path).values\n",
        "        except Exception as e:\n",
        "            return torch.zeros((1, 128, 256)), torch.zeros(6)\n",
        "\n",
        "        # 1. Handle NaNs and Log Scale\n",
        "        spectrogram = np.nan_to_num(spectrogram)\n",
        "        spectrogram = np.log1p(spectrogram)\n",
        "\n",
        "        # 2. THE FIX: CENTER CROP (Time Dimension)\n",
        "        # We want a fixed time window of 256 steps.\n",
        "        # If the file is 4000 steps long, we take the middle 256.\n",
        "        # If it's too short, we pad it.\n",
        "\n",
        "        desired_time = 256\n",
        "        current_time = spectrogram.shape[0] # The variable length (e.g. 337 or 4089)\n",
        "\n",
        "        if current_time > desired_time:\n",
        "            # Crop the center\n",
        "            start = (current_time - desired_time) // 2\n",
        "            spectrogram = spectrogram[start : start + desired_time, :]\n",
        "        else:\n",
        "            # Pad with zeros if too short\n",
        "            pad_needed = desired_time - current_time\n",
        "            # Pad (Bottom, Top) -> (Time_End, Time_Start)\n",
        "            spectrogram = np.pad(spectrogram, ((0, pad_needed), (0, 0)), mode='constant')\n",
        "\n",
        "        # 3. Resize Frequency (Height) only\n",
        "        # We can safely squish frequency (e.g. 400 -> 128) without losing the \"event\"\n",
        "        # cv2.resize takes (Width, Height) -> (Frequency, Time)\n",
        "        # We want output (256, 128) -> Time=256, Freq=128\n",
        "        spectrogram = cv2.resize(spectrogram, (128, 256))\n",
        "\n",
        "        # 4. Standardize\n",
        "        mean = spectrogram.mean()\n",
        "        std = spectrogram.std() + 1e-6\n",
        "        spectrogram = (spectrogram - mean) / std\n",
        "\n",
        "        # 5. Final Shape: (Channels, Time, Freq) -> (1, 256, 128)\n",
        "        # We need to Transpose so Time is the width?\n",
        "        # Actually EfficientNet expects (C, H, W). Let's treat Time as Width.\n",
        "        spectrogram = spectrogram.T # Now (128, 256) -> (Freq, Time)\n",
        "        spectrogram = torch.tensor(spectrogram, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Labels\n",
        "        label_cols = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
        "        labels = row[label_cols].values.astype('float32')\n",
        "        labels = labels / (labels.sum() + 1e-6)\n",
        "\n",
        "        return spectrogram, torch.tensor(labels, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "WXJCS58fFVLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ll2LEhUuTOqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-Transformer Hybrid"
      ],
      "metadata": {
        "id": "RkguP0z0TQgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 320 Channels"
      ],
      "metadata": {
        "id": "iUshtvOdUfB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes=6, d_model=256, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. The CNN Backbone (High Quality Mode)\n",
        "        self.cnn = timm.create_model(\n",
        "            'efficientnet_b0',\n",
        "            pretrained=True,\n",
        "            in_chans=1,\n",
        "            features_only=True,\n",
        "            # Index 4 is the deepest block in EfficientNet-B0\n",
        "            out_indices=[4]\n",
        "        )\n",
        "\n",
        "        # High Quality Configuration:\n",
        "        # The 5th block of EfficientNet-B0 outputs 320 channels.\n",
        "        cnn_out_channels = 320\n",
        "\n",
        "        # 2. Projection Layer\n",
        "        self.projection = nn.Linear(cnn_out_channels, d_model)\n",
        "\n",
        "        # 3. Positional Encoding\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 64, d_model))\n",
        "\n",
        "        # 4. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model*4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # 5. Classifier\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (Batch, 1, 128, 256)\n",
        "\n",
        "        # 1. CNN Feature Extract\n",
        "        # Returns the deep 320-channel features\n",
        "        features = self.cnn(x)[0]\n",
        "\n",
        "        # 2. Prepare for Transformer\n",
        "        # Pool Frequency dimension (Height) -> Leave Time (Width)\n",
        "        features = features.mean(dim=2)\n",
        "\n",
        "        # Permute: (Batch, Time, Channels)\n",
        "        features = features.permute(0, 2, 1)\n",
        "\n",
        "        # 3. Project (320 -> 256)\n",
        "        x = self.projection(features)\n",
        "\n",
        "        # 4. Positional Encoding\n",
        "        seq_len = x.shape[1]\n",
        "        if seq_len > self.pos_embedding.shape[1]:\n",
        "             x = x[:, :self.pos_embedding.shape[1], :]\n",
        "             seq_len = x.shape[1]\n",
        "\n",
        "        x = x + self.pos_embedding[:, :seq_len, :]\n",
        "\n",
        "        # 5. Transformer\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # 6. Classify\n",
        "        x = x.mean(dim=1)\n",
        "        output = self.classifier(x)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "nGNiyVN4UiuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- 1. Define paths confirming they point to local /content/ (on the Colab machine) ---\n",
        "# This is where the folders are unzipped\n",
        "TRAIN_IMG_DIR = '/content/train_spectrograms'\n",
        "VAL_IMG_DIR   = '/content/val_spectrograms'\n",
        "\n",
        "# You have the CSVs here (uploaded to Colab or Drive):\n",
        "TRAIN_CSV = '/content/drive/MyDrive/Seneca/Capstone/spectrograms/train.csv'\n",
        "VAL_CSV   = '/content/drive/MyDrive/Seneca/Capstone/spectrograms/val.csv'\n",
        "\n",
        "# --- 2. Create Datasets ---\n",
        "# HERE is where we pass the specific CSV file!\n",
        "train_dataset = HMSDataset(csv_file=TRAIN_CSV, img_dir=TRAIN_IMG_DIR)\n",
        "val_dataset   = HMSDataset(csv_file=VAL_CSV,   img_dir=VAL_IMG_DIR)\n",
        "\n",
        "# --- 3. THE OPTIMIZED LOADERS (The Batch Makers) ---\n",
        "# num_workers=2: Uses 2 background processes to load data (Speed boost!)\n",
        "# pin_memory=True: Speeds up the transfer from CPU RAM to GPU RAM\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,      # <--- CRITICAL FIX\n",
        "    pin_memory=True,    # <--- CRITICAL FIX\n",
        "    prefetch_factor=2   # <--- Loads 2 batches in advance\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,      # <--- CRITICAL FIX\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(train_dataset)} training samples.\")\n",
        "print(f\"Loaded {len(val_dataset)} validation samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk09_0E5ZEW7",
        "outputId": "70871aeb-2161-45c5-b619-a40d7a5d8e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 85440 training samples.\n",
            "Loaded 10680 validation samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BeInWedj_qof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training begins"
      ],
      "metadata": {
        "id": "UiTTy1Wm_vU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define the Device (GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "# 2. Instantiate the Model (The Brain)\n",
        "model = HybridModel(num_classes=6) # 6 classes for HMS dataset\n",
        "#model = HybridModel() # 6 classes for HMS dataset\n",
        "model = model.to(device) # Move brain to GPU\n",
        "\n",
        "# 3. Define the Optimizer (The Teacher's Feedback)\n",
        "# AdamW is the standard \"smart\" optimizer for Transformers\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# 4. Define the Loss Function (The Grading Scale)\n",
        "# Since we are using Soft Labels (probabilities like 0.2, 0.5), we use CrossEntropyLoss\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfORZFWWAh6_",
        "outputId": "0f1ac9c4-073b-4730-eb2f-b769f317fada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # This creates a nice progress bar\n",
        "\n",
        "def train_one_epoch(dataloader, model, loss_fn, optimizer, device):\n",
        "    model.train() # Set model to \"Learning Mode\" (enables Dropout, etc.)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # The Loop: Iterate over every batch of data\n",
        "    # tqdm wraps the loader to show a progress bar\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # A. Reset Gradients (Clear previous mistakes)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # B. Forward Pass (The Model guesses)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # C. Calculate Loss (Compare guess vs truth)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # D. Backward Pass (Calculate adjustments)\n",
        "        loss.backward()\n",
        "\n",
        "        # E. Optimize (Update weights)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of the score\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def validate_one_epoch(dataloader, model, loss_fn, device):\n",
        "    model.eval() # Set model to \"Test Mode\" (freezes layers)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad(): # Don't calculate gradients (saves memory)\n",
        "        for inputs, targets in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "oYvXvoayAtSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "NUM_EPOCHS = 10             # How many times to study the whole dataset\n",
        "best_val_loss = float('inf') # Track the best score to save the best model\n",
        "history = {'train_loss': [], 'val_loss': []} # To plot a graph later\n",
        "\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs on {device}...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- 1. Train (The Model Studies) ---\n",
        "    # Note: We use the function definitions I gave you earlier\n",
        "    train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer, device)\n",
        "\n",
        "    # --- 2. Validate (The Model Takes a Quiz) ---\n",
        "    val_loss = validate_one_epoch(val_loader, model, loss_fn, device)\n",
        "\n",
        "    # --- 3. Record History ---\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    # --- 4. Print Progress ---\n",
        "    # We print a clean line so you can see if it's improving\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # --- 5. Save the \"Best\" Version ---\n",
        "    # If the model scored better on the quiz than ever before, save it!\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
        "        print(f\"  --> New best model saved! (Loss: {val_loss:.4f})\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTraining Complete in {total_time/60:.2f} minutes!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9sNxV7KBqvZ",
        "outputId": "8dca9260-4214-40ef-aa10-12e5118de862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 1.1352 | Val Loss: 1.3700\n",
            "  --> New best model saved! (Loss: 1.3700)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 | Train Loss: 1.2988 | Val Loss: 1.3797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 | Train Loss: 1.3551 | Val Loss: 1.4273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       | 647/2670 [10:06<27:34,  1.22it/s, loss=1.41]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKFixoTYBoY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhd-RdYQRTyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_fRDLD0A4Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignore everything below this\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jctzJXD3RX4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already created 'train_loader' and 'val_loader' using the HMSDataset class\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # 1. Train\n",
        "    train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer, device)\n",
        "\n",
        "    # 2. Validate\n",
        "    val_loss = validate_one_epoch(val_loader, model, loss_fn, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # 3. Save the Best Model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
        "        print(\"  >>> New Best Model Saved!\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "id": "cT3MTNfMA4wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_OpTl86B-Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tf-JC7OAB957"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPORT YOUR CUSTOM CODE ---\n",
        "# These imports assume you created the files I gave you earlier\n",
        "from load_data import HMSDataset\n",
        "# Note: You need to create the model.py file with the 'HybridModel' class\n",
        "# I gave you in the previous step for this import to work.\n",
        "from app.model import HybridModel\n",
        "\n",
        "# --- CONFIGURATION (The Control Panel) ---\n",
        "DEBUG_MODE = True  # <--- SET THIS TO TRUE FOR MAC/WINDOWS TESTING\n",
        "                   # <--- SET THIS TO FALSE FOR ACTUAL GPU TRAINING\n",
        "\n",
        "BATCH_SIZE = 8     # Small batch size is safer for laptops\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 1 if DEBUG_MODE else 10\n",
        "DATA_PATH = \"./data\"\n",
        "CSV_FILE = os.path.join(DATA_PATH, \"train.csv\")\n",
        "SPECTROGRAM_PATH = os.path.join(DATA_PATH, \"train_spectrograms\")\n",
        "SAVE_PATH = \"./app/models/hms_model.pt\"\n",
        "\n",
        "def train():\n",
        "    # 1. Hardware Check\n",
        "    # This automatically picks GPU if available, otherwise CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"--- Running on: {device} ---\")\n",
        "    if DEBUG_MODE:\n",
        "        print(\"!! DEBUG MODE ACTIVE: Running on tiny dataset subset !!\")\n",
        "\n",
        "    # 2. Load Data\n",
        "    print(\"Initializing Dataset...\")\n",
        "    full_dataset = HMSDataset(CSV_FILE, SPECTROGRAM_PATH)\n",
        "\n",
        "    if DEBUG_MODE:\n",
        "        # If debugging, only grab the first 16 items (2 batches)\n",
        "        # This makes the \"epoch\" finish instantly\n",
        "        full_dataset.data = full_dataset.data.iloc[:16]\n",
        "\n",
        "    dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    print(f\"Data Loaded: {len(full_dataset)} samples\")\n",
        "\n",
        "    # 3. Initialize Model\n",
        "    print(\"Initializing Model...\")\n",
        "    model = HybridModel().to(device)\n",
        "\n",
        "    # 4. Define Loss & Optimizer\n",
        "    # KLDivLoss is standard for when your targets are probabilities (vote percentages)\n",
        "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 5. The Training Loop\n",
        "    print(\"\\nStarting Training...\")\n",
        "    model.train() # Set model to training mode (enables Dropout, etc.)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(dataloader):\n",
        "            # Move data to the same device as the model (CPU or GPU)\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Zero the gradients (reset from last step)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass (Guess)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Use LogSoftmax for KLDivLoss stability\n",
        "            outputs = torch.log_softmax(outputs, dim=1)\n",
        "\n",
        "            # Calculate Loss (Error)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass (Learn)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Print progress every 10 batches (or every batch in debug)\n",
        "            if DEBUG_MODE or (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # End of Epoch Stats\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}] Complete. Avg Loss: {epoch_loss:.4f}. Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    # 6. Save the Model\n",
        "    print(\"\\nSaving Model...\")\n",
        "    # Create the folder if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
        "    torch.save(model.state_dict(), SAVE_PATH)\n",
        "    print(f\"Model saved to {SAVE_PATH}\")\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "o9XWv0ZA_p1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}