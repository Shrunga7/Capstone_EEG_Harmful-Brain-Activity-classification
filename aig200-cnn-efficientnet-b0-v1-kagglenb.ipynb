{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass HMSSpectrogramDataset(Dataset):\n    def __init__(self, csv_file, spectrogram_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file (train.csv).\n            spectrogram_dir (string): Directory with all the .parquet files.\n            transform (callable, optional): Optional transform to be applied\n                on a sample (e.g., resizing, normalization).\n        \"\"\"\n        self.data = pd.read_csv(csv_file)\n        self.spectrogram_dir = spectrogram_dir\n        self.transform = transform\n\n        # The target columns (the votes for each class)\n        self.target_cols = [\n            'seizure_vote', 'lpd_vote', 'gpd_vote',\n            'lrda_vote', 'grda_vote', 'other_vote'\n        ]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 1. Get row information\n        row = self.data.iloc[idx]\n        spec_id = int(row['spectrogram_id'])\n        offset = int(row['spectrogram_label_offset_seconds'])\n\n        # 2. Construct file path\n        file_path = f\"{self.spectrogram_dir}/{spec_id}.parquet\"\n\n        # 3. Load the specific parquet file\n        # Note: In production, you might want to cache this or use a custom loader\n        # because reading Parquet repeatedly can be slow.\n        full_spectrogram = pd.read_parquet(file_path)\n\n        # 4. Slice the data based on the offset\n        # The spectrogram time starts at 0. We need the data starting at 'offset'.\n        # The competition usually assumes a 10-minute window (600 seconds).\n        # Since the rows in the parquet file typically represent 2 seconds each (0.5Hz),\n        # we need to be careful with indexing.\n        # *CHECK YOUR DATA*: Usually, the 'time' column in parquet aligns with offset.\n\n        # Select rows where 'time' is between offset and offset + 600\n        start_time = offset\n        end_time = offset + 600\n\n        # Filter the dataframe for the specific window\n        window_df = full_spectrogram.loc[\n            (full_spectrogram['time'] >= start_time) &\n            (full_spectrogram['time'] < end_time)\n        ]\n\n        # Drop the 'time' column as it's not a feature for the CNN\n        spec_data = window_df.drop(columns=['time']).values\n\n        # 5. Handle missing values (NaNs are common in this dataset)\n        # Replacing with 0 or the mean is standard.\n        # Log transformation is also very common for spectrograms to normalize peaks.\n        spec_data = np.nan_to_num(spec_data, nan=0.0)\n        spec_data = np.log1p(spec_data) # Log transform acts as normalization\n\n        # 6. Formatting for CNN: (Channels, Height, Width)\n        # Current shape: (Time_Steps, Frequencies) -> e.g., (300, 400)\n        # CNN expects: (1, 300, 400)\n        spec_tensor = torch.tensor(spec_data, dtype=torch.float32).unsqueeze(0)\n\n        # 7. Get the labels (targets)\n        # Normalize votes to probabilities (sum to 1) if they aren't already\n        votes = row[self.target_cols].values.astype(np.float32)\n        total_votes = votes.sum()\n        if total_votes > 0:\n            probabilities = votes / total_votes\n        else:\n            # Fallback if no votes (unlikely in clean data)\n            probabilities = np.zeros_like(votes)\n\n        target = torch.tensor(probabilities, dtype=torch.float32)\n\n        if self.transform:\n            spec_tensor = self.transform(spec_tensor)\n\n        return spec_tensor, target\n\nprint(\"Dataset Class Initialized\")\n\n# Usage Example:\n# dataset = HMSSpectrogramDataset(csv_file='train.csv', spectrogram_dir='./train_spectrograms')\n# loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T14:44:50.972585Z","iopub.execute_input":"2026-02-09T14:44:50.972907Z","iopub.status.idle":"2026-02-09T14:44:57.101679Z","shell.execute_reply.started":"2026-02-09T14:44:50.972871Z","shell.execute_reply":"2026-02-09T14:44:57.100684Z"}},"outputs":[{"name":"stdout","text":"Dataset Class Initialized\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm # PyTorch Image Models library - standard for Kaggle\n\nclass HybridSpectrogramModel(nn.Module):\n    def __init__(self, num_classes=6, d_model=256, nhead=4, num_layers=2):\n        super().__init__()\n\n        # 1. The CNN Backbone (Feature Extractor)\n        # We use EfficientNet-B0 because it's lightweight.\n        # 'features_only=True' is CRITICAL. It removes the final pooling/classifier layers.\n        self.cnn = timm.create_model(\n            'efficientnet_b0',\n            pretrained=True,\n            in_chans=1,             # Spectrograms are 1-channel (grayscale)\n            features_only=True,     # Return intermediate feature maps\n            out_indices=[4]         # Take the output from the last block\n        )\n\n        # EfficientNet-B0 output channels at the last block is 1280\n        cnn_out_channels = 320\n\n        # 2. Projection Layer\n        # Compresses the CNN features to the Transformer size (d_model)\n        self.projection = nn.Linear(cnn_out_channels, d_model)\n\n        # 3. Positional Encoding (Learnable)\n        # We assume a max sequence length (e.g., 32 chunks from the CNN)\n        self.pos_embedding = nn.Parameter(torch.randn(1, 64, d_model))\n\n        # 4. The Transformer Encoder (Lightweight!)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=d_model*4,\n            dropout=0.1,\n            batch_first=True # Expected input: (Batch, Seq_Len, Features)\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # 5. Final Classifier\n        self.classifier = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        # Input x shape: (Batch, 1, 128, 256) -> Example spectrogram size\n\n        # --- CNN Stage ---\n        # Output is a list of features. We took index [4], so we grab the last one.\n        # Shape: (Batch, 1280, H_small, W_small)\n        # Example: (Batch, 1280, 4, 8)\n        features = self.cnn(x)[0]\n\n        # --- Bridge Stage (The tricky part) ---\n        # We want to treat the \"Width\" of the image as \"Time\".\n        # We pool the \"Height\" (Frequency) because we want one vector per time step.\n\n        # Average over the frequency dimension (Height)\n        # Shape: (Batch, 1280, W_small)\n        features = features.mean(dim=2)\n\n        # Permute to put Time first for the Transformer? No, we used batch_first=True\n        # Desired Shape: (Batch, Time_Steps, Features) -> (Batch, W_small, 1280)\n        features = features.permute(0, 2, 1)\n\n        # Project to d_model (e.g., 1280 -> 256)\n        x = self.projection(features)\n\n        # Add Positional Embedding (broadcasts across batch)\n        # Ensure seq_len matches\n        seq_len = x.shape[1]\n        x = x + self.pos_embedding[:, :seq_len, :]\n\n        # --- Transformer Stage ---\n        # Shape: (Batch, Time_Steps, d_model)\n        x = self.transformer(x)\n\n        # --- Classification Stage ---\n        # We can take the mean of all time steps (Global Average Pooling)\n        # or just the last time step. Mean is usually more stable.\n        x = x.mean(dim=1)\n\n        # Output Logits\n        output = self.classifier(x)\n        return output\n\nprint(\"CNN Class Initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T14:48:16.430799Z","iopub.execute_input":"2026-02-09T14:48:16.431261Z","iopub.status.idle":"2026-02-09T14:48:26.074481Z","shell.execute_reply.started":"2026-02-09T14:48:16.431220Z","shell.execute_reply":"2026-02-09T14:48:26.073539Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"CNN Class Initialized\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Subset, random_split\nimport torch.optim as optim\n\n# --- CONFIGURATION ---\nNUM_SAMPLES = 100  # Total pool of data to draw from\nTRAIN_RATIO = 0.8  # 80% for training, 20% for validation\nBATCH_SIZE = 8\nEPOCHS = 10        # Increased slightly to see the trend\nLR = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 1. Initialize full dataset\nfull_dataset = HMSSpectrogramDataset(csv_file='/kaggle/input/hms-harmful-brain-activity-classification/train.csv', spectrogram_dir='/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms')\n\n# 2. Subset first, then split\n# This allows you to control the total data used (100, 500, etc.)\nindices = np.arange(NUM_SAMPLES)\nmain_subset = Subset(full_dataset, indices)\n\ntrain_size = int(TRAIN_RATIO * len(main_subset))\nval_size = len(main_subset) - train_size\n\ntrain_dataset, val_dataset = random_split(main_subset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# 3. Model, Loss, Optimizer\nmodel = HybridSpectrogramModel(num_classes=6).to(DEVICE)\ncriterion = nn.KLDivLoss(reduction='batchmean') #Loss f'n\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\n# --- TRAINING & VALIDATION LOOP ---\nfor epoch in range(EPOCHS):\n    # Training Phase\n    model.train()\n    train_loss = 0.0\n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(torch.log_softmax(outputs, dim=1), targets)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    # Validation Phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            outputs = model(inputs)\n            loss = criterion(torch.log_softmax(outputs, dim=1), targets)\n            val_loss += loss.item()\n\n    avg_train = train_loss / len(train_loader)\n    avg_val = val_loss / len(val_loader)\n    \n    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}\")\n\nprint(\"Training cycle with validation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T14:48:31.173886Z","iopub.execute_input":"2026-02-09T14:48:31.174215Z","iopub.status.idle":"2026-02-09T14:53:22.778642Z","shell.execute_reply.started":"2026-02-09T14:48:31.174189Z","shell.execute_reply":"2026-02-09T14:53:22.777700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed9b0688789a4578ba061d027372de18"}},"metadata":{}},{"name":"stderr","text":"Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] | Train Loss: 1.1429 | Val Loss: 2.4756\nEpoch [2/10] | Train Loss: 0.9497 | Val Loss: 0.8215\nEpoch [3/10] | Train Loss: 0.6774 | Val Loss: 0.4389\nEpoch [4/10] | Train Loss: 0.4153 | Val Loss: 0.4019\nEpoch [5/10] | Train Loss: 0.3580 | Val Loss: 0.2348\nEpoch [6/10] | Train Loss: 0.3101 | Val Loss: 0.2670\nEpoch [7/10] | Train Loss: 0.3238 | Val Loss: 0.7150\nEpoch [8/10] | Train Loss: 0.5272 | Val Loss: 0.3126\nEpoch [9/10] | Train Loss: 0.2847 | Val Loss: 0.2772\nEpoch [10/10] | Train Loss: 0.6591 | Val Loss: 0.2705\nTraining cycle with validation complete!\n","output_type":"stream"}],"execution_count":3}]}